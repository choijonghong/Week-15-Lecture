# Week-15-Lecture
Week-15-Lecture

### AI는 어떻게 ‘글만 보고’ 이미지를 만들 수 있을까?”
* 논문명: Zero-Shot Text-to-Image  Generation
* AI가 문장만 보고 그림을 자동으로 생성하는 기술 — 이 논문이 그 비밀을 풀어줍니다.
  

### 논문배경 - 왜 이런 연구가 필요했을까?
* 세상의 모든 사물에 일일이 라벨을 붙여 가르치는 건 불가능합니다.
* 그래서 인터넷의 방대한 데이터를 가공 없이 학습할 방법이 필요했습니다.
* 배운 적 없는 '새로운 개념'도 스스로 추론해 그려내기 위해서입니다.
* "아코디언 기린" 같은 인간의 엉뚱한 상상력을 현실화하기 위함입니다.
* 복잡한 설정 없이, 사람의 말(자연어)을 바로 이해하도록 만들기 위해서입니다.
* 특정 전문가뿐 아니라 누구나 쉽게 쓸 수 있는 범용 도구가 필요했습니다.
* 결국 AI가 단순한 '모방'을 넘어 진정한 '창작'을 하도록 만들기 위함입니다.

### 기존 연구의 한계점
* GAN과 VAE 기반 모델의 한계
  * 기존 모델은 미리 배운 특정 주제만 그릴 수 있었습니다.(예:얼굴만 잘 그리는 모델, 꽃만 그리는 모델)
  * 구조가 복잡해서 학습시키기가 매우 까다롭고 불안정했습니다.
  * 만들어낸 이미지가 흐릿하거나 똑같은 그림만 반복하곤 했습니다.
  * 사람의 긴 설명이나 복잡한 문장은 제대로 이해하지 못했습니다.
  * 배우지 않은 새로운 것을 그려달라고 하면 전혀 대응하지 못했습니다.

### 논문 목표
* 복잡한 특화 기술들을 모두 버리고, 거대한 트랜스포머(Transformer) 하나로 텍스트와 이미지를 통합하여 마치 언어처럼 학습시키는 것입니다.
* 이를 통해 사람이 정답을 알려주지 않아도, 배운 적 없는 낯선 문장까지 스스로 이해하고 그려내는 강력한 범용성(Zero-shot)을 확보하는 것입니다.

### 논문내용(주요 혁신 및 방법론)  

"순서대로 기다리지 않고 문장 전체를 한눈에 파악하며(병렬 처리), 여러 개의 눈으로 단어 사이의 미세한 관계까지 놓치지 않는(Multi-Head Attention) 혁신적인 모델입니다."

<img width="300" height="500" alt="image" src="https://github.com/user-attachments/assets/c76fc4b4-bb7c-4f88-80e3-7a4b8a79dd03" />


* 위치 정보 주입: Positional Encoding
  * Transformer는 문장을 처음부터 끝까지 순서대로 읽지 않습니다.
  * 문장 전체를 한 번에 입력받기 때문에 단어의 순서를 모릅니다.
  * 그래서 각 단어에 '순서 번호'를 매겨주는 수학적 신호를 추가합니다.

* 핵심 원리: Self-Attention (자기 주의 집중)
  * 단어의 의미는 문맥에 따라 결정됩니다. (예: '배'는 먹는 배? 타는 배?)
  * 이 모델은 문장 내의 모든 단어가 서로를 쳐다보게 합니다.
  * 각 단어가 서로 얼마나 밀접한 관계인지 계산하여 문맥을 파악합니다.

* 계산 방식: Scaled Dot-Product Attention
  *   <img width="199" height="199" alt="image" src="https://github.com/user-attachments/assets/4df60418-b30e-49e1-a0ff-3196e9bd76c6" />

  * 단어 간의 관계를 수학적으로 계산(내적)하는 구체적인 방법입니다.
  * 값이 너무 커지면 학습이 불안정해질 수 있습니다.
  * 따라서 계산 값을 적절한 크기로 나누어(Scaling) 빠르고 안정적인 학습을 돕습니다.

* 다각도 분석: Multi-Head Attention
  *   <img width="213" height="221" alt="image" src="https://github.com/user-attachments/assets/b4136df2-fabb-4f3e-85fc-2c99ff251b44" />

  * 문장을 한 가지 관점으로만 보면 놓치는 정보가 생깁니다.
  * 여러 개의 Attention(헤드)을 두어 동시에 여러 관점으로 문장을 분석합니다.
  * 어떤 헤드는 문법을, 어떤 헤드는 의미를 파악하여 이해력을 높입니다.

* 효율성: 완전 병렬 구조
  * 앞 단어의 계산이 끝날 때까지 기다릴 필요가 없습니다.
  * 모든 단어의 관계를 동시에 계산합니다.
  * GPU의 능력을 100% 활용하여 학습 속도가 기존보다 압도적으로 빠릅니다.

### 기여도 및 결과
* Transformer는 영→독, 영→불 번역에서 기존 최고 성능 모델을 모두 넘어서는 성과를 기록했습니다.
* 기존 RNN·CNN 기반 모델은 순차 처리 때문에 시간이 오래 걸렸습니다, 하지만 Transformer는 병렬 처리로 학습 시간을 대폭 단축했습니다.
* Transformer는 RNN과 CNN 없이 Attention만으로 언어를 이해하고 생성할 수 있음을 증명했습니다.
* 이 구조는 이후 BERT, GPT, DALL·E 등 모든 생성형 AI의 기본 설계가 되었습니다.

### 감성컴퓨팅과 GAN
* 감성컴퓨팅은 사람의 감정·뉘앙스·태도 같은 비언어적 의미를 이해하는 AI를 목표로 한다.
* 기존 RNN 기반 모델은 문맥의 미세한 변화나 긴 대화 맥락에서의 감정 흐름 파악이 제한적이었다.
* Transformer의 Self-Attention은 문장 전체 단어를 동시에 비교하여 감정의 전환, 강조, 반전 등을 정밀하게 탐지할 수 있게 한다.
* Multi-Head Attention은 감정·주제·대상·의도 등 복수의 의미 층을 동시에 해석하는 데 유리하다.
* 그 결과, Transformer는 감성 분석, 감정 기반 추천, 상담 챗봇, 생성형 대화 등 감성컴퓨팅 핵심 분야의 정확도와 자연스러움을 크게 향상시켰다.
* 요약하면, Transformer는 문장 전체를 바라보며 의미를 분석하는 구조 덕분에, 감정과 뉘앙스처럼 미세한 맥락을 이해해야 하는 감성컴퓨팅에 최적화된 기반 기술이다.

### 참고 파일

* 논문1. Zero-Shot Text-to-Image  Generation

